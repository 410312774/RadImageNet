{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "from collections import OrderedDict\n",
    "from torchvision.models import resnet50, densenet121, inception_v3\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('dataframe/train_fold"+str(i+1)+".csv')\n",
    "df_val = pd.read_csv('dataframe/val_fold"+str(i+1)+".csv')\n",
    "df_test= pd.read_csv('dataframe/test_fold"+str(i+1)+".csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(x):\n",
    "    if x == 'yes':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['label']=df_train['label'].apply(get_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['label']=df_val['label'].apply(get_label)\n",
    "df_test['label']=df_test['label'].apply(get_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = len(df_train.label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_class):\n",
    "        super().__init__()\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.linear = nn.Linear(2048, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.drop_out(x)\n",
    "        x = self.linear(x)\n",
    "        #x = torch.softmax(x, dim=-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        base_model = resnet50(pretrained=False)\n",
    "        encoder_layers = list(base_model.children())\n",
    "        self.backbone = nn.Sequential(*encoder_layers[:9])\n",
    "                        \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = Backbone()\n",
    "classifier = Classifier(num_class=num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backbone.load_state_dict(torch.load(\"/raid/data/yanglab/rin3d_downstream/zelong/output/resnet_rin2d/backbone_best_accuracy.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#classifier.load_state_dict(torch.load(\"/raid/data/yanglab/rin3d_downstream/zelong/output/resnet_rin2d/classifier_best_accuracy.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(backbone, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = nn.DataParallel(model, device_ids=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class createDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataframe.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image = self.dataframe.iloc[index][\"img_dir\"]\n",
    "        image = cv2.imread(image)\n",
    "        image = (image-127.5)*2 / 255\n",
    "        image = cv2.resize(image,(224,224))\n",
    "        #image = np.transpose(image,(2,0,1))   \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        label = self.dataframe.iloc[index][\"label\"]\n",
    "        return {\"image\": image , \"label\": torch.tensor(label, dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = createDataset(df_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=20)\n",
    "val_dataset = createDataset(df_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=20)\n",
    "\n",
    "test_dataset = createDataset(df_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer,  num_epochs=30):\n",
    "    min_valid_loss = np.inf\n",
    "\n",
    "    for e in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        model.train()     # Optional when not using Model Specific layer\n",
    "        for i_batch, info_batch in enumerate(train_loader):\n",
    "            if torch.cuda.is_available():\n",
    "                data, labels = info_batch['image'].to(device, dtype=torch.float), info_batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            target = model(data)\n",
    "            loss = criterion(target,labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        valid_loss = 0.0\n",
    "        model.eval()     # Optional when not using Model Specific layer\n",
    "        for i_batch, info_batch in enumerate(val_loader):\n",
    "            if torch.cuda.is_available():\n",
    "                data, labels = info_batch['image'].to(device, dtype=torch.float), info_batch['label'].to(device)\n",
    "            \n",
    "            target = model(data)\n",
    "            loss = criterion(target,labels)\n",
    "            valid_loss = loss.item() * data.size(0)\n",
    "\n",
    "        print(f'Epoch {e+1} \\t\\t Training Loss: {train_loss / len(train_loader)} \\t\\t Validation Loss: {valid_loss / len(val_loader)}')\n",
    "        if min_valid_loss > valid_loss:\n",
    "            print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
    "            min_valid_loss = valid_loss\n",
    "            # Saving State Dict\n",
    "            torch.save(model.state_dict(), 'acl_fold1_best_model.pth')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t\t Training Loss: 0.4457762529561808 \t\t Validation Loss: 0.31227233193137427\n",
      "Validation Loss Decreased(inf--->3.434996) \t Saving The Model\n",
      "Epoch 2 \t\t Training Loss: 0.14679992552027926 \t\t Validation Loss: 0.09849888628179376\n",
      "Validation Loss Decreased(3.434996--->1.083488) \t Saving The Model\n",
      "Epoch 3 \t\t Training Loss: 0.08154763455744475 \t\t Validation Loss: 0.12573414499109442\n",
      "Epoch 4 \t\t Training Loss: 0.062108178586560904 \t\t Validation Loss: 0.25889845327897504\n",
      "Epoch 5 \t\t Training Loss: 0.06391522991675679 \t\t Validation Loss: 0.017163275317712265\n",
      "Validation Loss Decreased(1.083488--->0.188796) \t Saving The Model\n",
      "Epoch 6 \t\t Training Loss: 0.07031963857658051 \t\t Validation Loss: 0.03573957085609436\n",
      "Epoch 7 \t\t Training Loss: 0.06533189754890582 \t\t Validation Loss: 0.18448266116055576\n",
      "Epoch 8 \t\t Training Loss: 0.018094773113741643 \t\t Validation Loss: 0.0021281332116235385\n",
      "Validation Loss Decreased(0.188796--->0.023409) \t Saving The Model\n",
      "Epoch 9 \t\t Training Loss: 0.014920052743616328 \t\t Validation Loss: 0.002436336637897925\n",
      "Epoch 10 \t\t Training Loss: 0.006788608369298834 \t\t Validation Loss: 0.007084378464655442\n",
      "Epoch 11 \t\t Training Loss: 0.0015907104557100155 \t\t Validation Loss: 0.24107835509560324\n",
      "Epoch 12 \t\t Training Loss: 0.0032665943758079913 \t\t Validation Loss: 0.0003260832533917644\n",
      "Validation Loss Decreased(0.023409--->0.003587) \t Saving The Model\n",
      "Epoch 13 \t\t Training Loss: 0.0016831970993675047 \t\t Validation Loss: 0.7905788421630859\n",
      "Epoch 14 \t\t Training Loss: 0.0017326752802416326 \t\t Validation Loss: 0.008139228278940374\n",
      "Epoch 15 \t\t Training Loss: 0.00318281331147863 \t\t Validation Loss: 0.28556355563077057\n",
      "Epoch 16 \t\t Training Loss: 0.007273598028962294 \t\t Validation Loss: 0.4322407895868475\n",
      "Epoch 17 \t\t Training Loss: 0.0029323149308586103 \t\t Validation Loss: 0.6556116884404962\n",
      "Epoch 18 \t\t Training Loss: 0.08122620849541648 \t\t Validation Loss: 0.08495957201177423\n",
      "Epoch 19 \t\t Training Loss: 0.04737511703969686 \t\t Validation Loss: 0.18606922843239523\n",
      "Epoch 20 \t\t Training Loss: 0.0035187961037501316 \t\t Validation Loss: 0.1872019334272905\n",
      "Epoch 21 \t\t Training Loss: 0.0008831469497874752 \t\t Validation Loss: 0.0005289917303757234\n",
      "Epoch 22 \t\t Training Loss: 0.0006706713745106154 \t\t Validation Loss: 0.016736537218093872\n",
      "Epoch 23 \t\t Training Loss: 0.013629328110075402 \t\t Validation Loss: 0.6344210451299493\n",
      "Epoch 24 \t\t Training Loss: 0.023848454354840093 \t\t Validation Loss: 0.444000244140625\n",
      "Epoch 25 \t\t Training Loss: 0.004152694679510455 \t\t Validation Loss: 0.02475080977786671\n",
      "Epoch 26 \t\t Training Loss: 0.00046661380891082146 \t\t Validation Loss: 0.038509940559213814\n",
      "Epoch 27 \t\t Training Loss: 0.11307178725020434 \t\t Validation Loss: 0.7771469463001598\n",
      "Epoch 28 \t\t Training Loss: 0.012336848162074322 \t\t Validation Loss: 0.2670136581767689\n",
      "Epoch 29 \t\t Training Loss: 0.0023676027523314197 \t\t Validation Loss: 0.33774974129416724\n",
      "Epoch 30 \t\t Training Loss: 0.003787129033608525 \t\t Validation Loss: 0.6296653747558594\n"
     ]
    }
   ],
   "source": [
    "best_model = train_model(model, criterion,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as nnf\n",
    "from sklearn.metrics import roc_auc_score,roc_curve,auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, testloader):\n",
    "    \"\"\"\n",
    "    Function to test the model\n",
    "    \"\"\"\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "    pred_list= []\n",
    "    print('Testing')\n",
    "    valid_running_correct = 0\n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "            counter += 1\n",
    "            image = data['image']\n",
    "            labels = data['label']\n",
    "            #image, labels = data\n",
    "            image = image.to(device, dtype=torch.float)\n",
    "            labels = labels.to(device, dtype=torch.float)\n",
    "            # forward pass\n",
    "            outputs = model(image)\n",
    "            # calculate the accuracy\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            prob = nnf.softmax(outputs, dim=1)\n",
    "            prob = prob.cpu().detach().numpy().tolist()\n",
    "            pred_list.append(prob[0][1])\n",
    "            valid_running_correct += (preds == labels).sum().item()\n",
    "        \n",
    "    # loss and accuracy for the complete epoch\n",
    "    final_acc = 100. * (valid_running_correct / len(testloader.dataset))\n",
    "    print(final_acc)\n",
    "    return pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 208/208 [00:02<00:00, 79.39it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.23076923076923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "list1= test(best_model,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9827181365642904"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr, tpr, threshold = roc_curve(df_test.label.tolist(),list1)\n",
    "#print(fpr, tpr)\n",
    "auc_ = auc(fpr, tpr)\n",
    "auc_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9827181365642904"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(df_test.label.tolist(),list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.722537040710449,\n",
       " 5.256601810455322,\n",
       " 6.35347318649292,\n",
       " 8.303277969360352,\n",
       " 6.335359573364258,\n",
       " 15.81423282623291,\n",
       " 12.608792304992676,\n",
       " 15.327881813049316,\n",
       " 7.674429893493652,\n",
       " 8.31491756439209,\n",
       " 11.802374839782715,\n",
       " 4.572198867797852,\n",
       " 5.167838096618652,\n",
       " 4.5743279457092285,\n",
       " 0.13554474711418152,\n",
       " 4.950382232666016,\n",
       " 5.77001953125,\n",
       " 7.225028991699219,\n",
       " 4.258795261383057,\n",
       " 5.859421253204346,\n",
       " 9.435050010681152,\n",
       " 11.51740837097168,\n",
       " 4.866803169250488,\n",
       " 5.880049228668213,\n",
       " -2.0308358669281006,\n",
       " 0.1658126711845398,\n",
       " 5.12605094909668,\n",
       " 5.229898929595947,\n",
       " -5.735682010650635,\n",
       " -5.506959438323975,\n",
       " 0.7582612037658691,\n",
       " -4.412546157836914,\n",
       " -2.5494024753570557,\n",
       " 2.1737048625946045,\n",
       " 2.30580735206604,\n",
       " 2.3985064029693604,\n",
       " 0.8194136023521423,\n",
       " 8.989350318908691,\n",
       " 6.967096328735352,\n",
       " 4.861917495727539,\n",
       " 6.991560935974121,\n",
       " 8.723336219787598,\n",
       " 8.259665489196777,\n",
       " 6.969813823699951,\n",
       " 8.246603965759277,\n",
       " 6.46829080581665,\n",
       " 8.153017044067383,\n",
       " 4.003452301025391,\n",
       " 4.458338260650635,\n",
       " 2.7990386486053467,\n",
       " 5.206474781036377,\n",
       " 9.004145622253418,\n",
       " 9.931026458740234,\n",
       " 7.609230041503906,\n",
       " 4.370948314666748,\n",
       " 9.221497535705566,\n",
       " 4.3663649559021,\n",
       " 6.430750846862793,\n",
       " -7.442127227783203,\n",
       " -6.533224582672119,\n",
       " -5.4069719314575195,\n",
       " -8.162598609924316,\n",
       " -7.6902031898498535,\n",
       " -4.7786149978637695,\n",
       " -4.692056655883789,\n",
       " -5.120092391967773,\n",
       " -4.5998406410217285,\n",
       " -4.939715385437012,\n",
       " -4.506278991699219,\n",
       " -7.3282551765441895,\n",
       " -4.033483982086182,\n",
       " -6.168396949768066,\n",
       " -5.493012428283691,\n",
       " -5.149848461151123,\n",
       " -5.6540374755859375,\n",
       " -5.910941123962402,\n",
       " -5.390017032623291,\n",
       " -6.123354911804199,\n",
       " -8.174957275390625,\n",
       " -8.297874450683594,\n",
       " 0.8374060392379761,\n",
       " 0.7969570159912109,\n",
       " -8.12879753112793,\n",
       " -8.878751754760742,\n",
       " -3.363637685775757,\n",
       " -5.89668607711792,\n",
       " -6.887218952178955,\n",
       " -5.317153453826904,\n",
       " -6.467265605926514,\n",
       " 2.7544260025024414,\n",
       " -6.674293041229248,\n",
       " -3.155958890914917,\n",
       " -4.661300182342529,\n",
       " -6.052978038787842,\n",
       " -7.0799736976623535,\n",
       " -6.799651145935059,\n",
       " -5.833767890930176,\n",
       " -2.749861478805542,\n",
       " -5.107069492340088,\n",
       " -6.783008575439453,\n",
       " -6.516847610473633,\n",
       " -5.645150661468506,\n",
       " -4.452874183654785,\n",
       " -6.584769248962402,\n",
       " -4.908306121826172,\n",
       " -5.22123908996582,\n",
       " -6.1670241355896,\n",
       " -5.832910060882568,\n",
       " -4.46212100982666,\n",
       " -4.860372066497803,\n",
       " -4.646491050720215,\n",
       " -3.1085777282714844,\n",
       " -5.106777667999268,\n",
       " -6.66447639465332,\n",
       " -2.947197675704956,\n",
       " -6.382071018218994,\n",
       " -3.794668436050415,\n",
       " -4.245490074157715,\n",
       " -4.571384429931641,\n",
       " -4.320315837860107,\n",
       " -4.517399787902832,\n",
       " -4.80472993850708,\n",
       " -3.7977566719055176,\n",
       " -2.6185708045959473,\n",
       " -4.057940483093262,\n",
       " -4.154492378234863,\n",
       " -7.211996555328369,\n",
       " -6.413490295410156,\n",
       " -7.963860034942627,\n",
       " -4.155766010284424,\n",
       " -7.974860668182373,\n",
       " -7.420297622680664,\n",
       " -0.6388338804244995,\n",
       " 7.195127487182617,\n",
       " 7.767790794372559,\n",
       " 8.257777214050293,\n",
       " 9.733378410339355,\n",
       " 6.479293346405029,\n",
       " 5.735156059265137,\n",
       " 8.169873237609863,\n",
       " 4.808221340179443,\n",
       " 7.159987926483154,\n",
       " 9.71696662902832,\n",
       " 4.452095031738281,\n",
       " 1.0383366346359253,\n",
       " 1.2208940982818604,\n",
       " -0.4142036437988281,\n",
       " 3.4030890464782715,\n",
       " 5.425745964050293,\n",
       " 6.433385848999023,\n",
       " -0.5583215355873108,\n",
       " 0.6417710185050964,\n",
       " 5.89835786819458,\n",
       " 12.115497589111328,\n",
       " 10.32947826385498,\n",
       " 11.55933666229248,\n",
       " 6.611637592315674,\n",
       " 5.995724678039551,\n",
       " 6.1437225341796875,\n",
       " 6.4153218269348145,\n",
       " 6.658017635345459,\n",
       " 12.17837142944336,\n",
       " 7.170639991760254,\n",
       " 10.874349594116211,\n",
       " 7.358556270599365,\n",
       " 7.8041791915893555,\n",
       " 9.189516067504883,\n",
       " 7.296512603759766,\n",
       " -3.6374340057373047,\n",
       " 1.6512895822525024,\n",
       " -3.574918508529663,\n",
       " -3.114183187484741,\n",
       " 8.845585823059082,\n",
       " 8.7772855758667,\n",
       " 6.948025226593018,\n",
       " 7.262937545776367,\n",
       " 6.357936859130859,\n",
       " 4.751142978668213,\n",
       " 6.46080207824707,\n",
       " 6.299152851104736,\n",
       " 7.206450462341309,\n",
       " 8.072981834411621,\n",
       " 9.482826232910156,\n",
       " 6.921614170074463,\n",
       " 10.325295448303223,\n",
       " 3.5427424907684326,\n",
       " 3.994328498840332,\n",
       " 8.307639122009277,\n",
       " 8.138629913330078,\n",
       " 8.39436149597168,\n",
       " 10.77093505859375,\n",
       " 9.935171127319336,\n",
       " 8.40749740600586,\n",
       " 9.359107971191406,\n",
       " 6.789090633392334,\n",
       " -2.811217784881592,\n",
       " -1.3526970148086548,\n",
       " -4.648891448974609,\n",
       " -3.845475912094116,\n",
       " -2.1099681854248047,\n",
       " -6.082991123199463,\n",
       " -4.225076675415039,\n",
       " -5.114105701446533,\n",
       " -4.771008491516113,\n",
       " -6.266976833343506,\n",
       " -5.937575817108154,\n",
       " -6.137179374694824,\n",
       " -4.466598033905029]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output: torch.Tensor, target: torch.Tensor, topk=(1,)) -> List[torch.FloatTensor]:\n",
    "    \"\"\"\n",
    "    Computes the accuracy over the k top predictions for the specified values of k\n",
    "    In top-5 accuracy you give yourself credit for having the right answer\n",
    "    if the right answer appears in your top five guesses.\n",
    "\n",
    "    ref:\n",
    "    - https://pytorch.org/docs/stable/generated/torch.topk.html\n",
    "    - https://discuss.pytorch.org/t/imagenet-example-accuracy-calculation/7840\n",
    "    - https://gist.github.com/weiaicunzai/2a5ae6eac6712c70bde0630f3e76b77b\n",
    "    - https://discuss.pytorch.org/t/top-k-error-calculation/48815/2\n",
    "    - https://stackoverflow.com/questions/59474987/how-to-get-top-k-accuracy-in-semantic-segmentation-using-pytorch\n",
    "\n",
    "    :param output: output is the prediction of the model e.g. scores, logits, raw y_pred before normalization or getting classes\n",
    "    :param target: target is the truth\n",
    "    :param topk: tuple of topk's to compute e.g. (1, 2, 5) computes top 1, top 2 and top 5.\n",
    "    e.g. in top 2 it means you get a +1 if your models's top 2 predictions are in the right label.\n",
    "    So if your model predicts cat, dog (0, 1) and the true label was bird (3) you get zero\n",
    "    but if it were either cat or dog you'd accumulate +1 for that example.\n",
    "    :return: list of topk accuracy [top1st, top2nd, ...] depending on your topk input\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # ---- get the topk most likely labels according to your model\n",
    "        # get the largest k \\in [n_classes] (i.e. the number of most likely probabilities we will use)\n",
    "        maxk = max(topk)  # max number labels we will consider in the right choices for out model\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        # get top maxk indicies that correspond to the most likely probability scores\n",
    "        # (note _ means we don't care about the actual top maxk scores just their corresponding indicies/labels)\n",
    "        _, y_pred = output.topk(k=maxk, dim=1)  # _, [B, n_classes] -> [B, maxk]\n",
    "        y_pred = y_pred.t()  # [B, maxk] -> [maxk, B] Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.\n",
    "\n",
    "        # - get the credit for each example if the models predictions is in maxk values (main crux of code)\n",
    "        # for any example, the model will get credit if it's prediction matches the ground truth\n",
    "        # for each example we compare if the model's best prediction matches the truth. If yes we get an entry of 1.\n",
    "        # if the k'th top answer of the model matches the truth we get 1.\n",
    "        # Note: this for any example in batch we can only ever get 1 match (so we never overestimate accuracy <1)\n",
    "        target_reshaped = target.view(1, -1).expand_as(y_pred)  # [B] -> [B, 1] -> [maxk, B]\n",
    "        # compare every topk's model prediction with the ground truth & give credit if any matches the ground truth\n",
    "        correct = (y_pred == target_reshaped)  # [maxk, B] were for each example we know which topk prediction matched truth\n",
    "        # original: correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        # -- get topk accuracy\n",
    "        list_topk_accs = []  # idx is topk1, topk2, ... etc\n",
    "        for k in topk:\n",
    "            # get tensor of which topk answer was right\n",
    "            ind_which_topk_matched_truth = correct[:k]  # [maxk, B] -> [k, B]\n",
    "            # flatten it to help compute if we got it correct for each example in batch\n",
    "            flattened_indicator_which_topk_matched_truth = ind_which_topk_matched_truth.reshape(-1).float()  # [k, B] -> [kB]\n",
    "            # get if we got it right for any of our top k prediction for each example in batch\n",
    "            tot_correct_topk = flattened_indicator_which_topk_matched_truth.float().sum(dim=0, keepdim=True)  # [kB] -> [1]\n",
    "            # compute topk accuracy - the accuracy of the mode's ability to get it right within it's top k guesses/preds\n",
    "            topk_acc = tot_correct_topk / batch_size  # topk accuracy for entire batch\n",
    "            list_topk_accs.append(topk_acc)\n",
    "        return list_topk_accs  # list of topk accuracies for entire batch [topk1, topk2, ... etc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "        # measure data loading time\n",
    "        print(f\"Processing {batch_idx+1}/{len(test_loader)}\")\n",
    "        image = inputs.to(device, dtype=torch.float)\n",
    "        labels = targets.to(device, dtype=torch.float)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(image)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, labels.data, topk=(1, 5))\n",
    "        print(prec1,prec5)\n",
    "        top1.update(prec1.item(), inputs.size(0))\n",
    "        top5.update(prec5.item(), inputs.size(0))\n",
    "\n",
    "print(top1)\n",
    "print(top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-b4ecf18e440e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker_queue_idx_cycle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;31m# No certainty which module multiprocessing_context is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker_result_queue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQueue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[var-annotated]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker_pids_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/context.py\u001b[0m in \u001b[0;36mQueue\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;34m'''Returns a queue object'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mqueues\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQueue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mQueue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mJoinableQueue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/queues.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, maxsize, ctx)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wlock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wlock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoundedSemaphore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# For use by concurrent.futures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/context.py\u001b[0m in \u001b[0;36mLock\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;34m'''Returns a non-recursive lock object'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mRLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/synchronize.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mSemLock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSEMAPHORE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/synchronize.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, kind, value, maxvalue, ctx)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 sl = self._semlock = _multiprocessing.SemLock(\n\u001b[0m\u001b[1;32m     58\u001b[0m                     \u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     unlink_now)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "for batch in test_loader:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-e00fd4d3e26f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m106210\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-93-d9d53ed9ab2c>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m127.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m#image = np.transpose(image,(2,0,1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/skimage/transform/_warps.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(image, output_shape, order, mode, cval, clip, preserve_range, anti_aliasing, anti_aliasing_sigma)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mtform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAffineTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0mtform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_corners\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_corners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;31m# Make sure the transform is exactly metric, to ensure fast warping.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/skimage/transform/_geometric.py\u001b[0m in \u001b[0;36mestimate\u001b[0;34m(self, src, dst)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;31m# De-center and de-normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m         \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst_matrix\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0msrc_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36minv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m     \u001b[0mainv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_loader.dataset[106210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = data['image']\n",
    "            labels = data['label']\n",
    "            #image, labels = data\n",
    "            image = image.to(device, dtype=torch.float)\n",
    "            labels = labels.to(device, dtype=torch.float)\n",
    "            # forward pass\n",
    "            outputs = model(image)\n",
    "            # calculate the accuracy\n",
    "            _, preds = torch.max(outputs.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/163796 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': tensor([[[[-0.8420, -0.8420, -0.8394,  ..., -0.8231, -0.8268, -0.8269],\n",
      "          [-0.8257, -0.8241, -0.8235,  ..., -0.7942, -0.8152, -0.8179],\n",
      "          [-0.8080, -0.8034, -0.8006,  ..., -0.7534, -0.7922, -0.8004],\n",
      "          ...,\n",
      "          [-0.8431, -0.8431, -0.8403,  ..., -0.8353, -0.8353, -0.8353],\n",
      "          [-0.8431, -0.8431, -0.8403,  ..., -0.8353, -0.8353, -0.8353],\n",
      "          [-0.8431, -0.8431, -0.8403,  ..., -0.8353, -0.8353, -0.8353]],\n",
      "\n",
      "         [[-0.8420, -0.8420, -0.8394,  ..., -0.8231, -0.8268, -0.8269],\n",
      "          [-0.8257, -0.8241, -0.8235,  ..., -0.7942, -0.8152, -0.8179],\n",
      "          [-0.8080, -0.8034, -0.8006,  ..., -0.7534, -0.7922, -0.8004],\n",
      "          ...,\n",
      "          [-0.8431, -0.8431, -0.8403,  ..., -0.8353, -0.8353, -0.8353],\n",
      "          [-0.8431, -0.8431, -0.8403,  ..., -0.8353, -0.8353, -0.8353],\n",
      "          [-0.8431, -0.8431, -0.8403,  ..., -0.8353, -0.8353, -0.8353]],\n",
      "\n",
      "         [[-0.8420, -0.8420, -0.8394,  ..., -0.8231, -0.8268, -0.8269],\n",
      "          [-0.8257, -0.8241, -0.8235,  ..., -0.7942, -0.8152, -0.8179],\n",
      "          [-0.8080, -0.8034, -0.8006,  ..., -0.7534, -0.7922, -0.8004],\n",
      "          ...,\n",
      "          [-0.8431, -0.8431, -0.8403,  ..., -0.8353, -0.8353, -0.8353],\n",
      "          [-0.8431, -0.8431, -0.8403,  ..., -0.8353, -0.8353, -0.8353],\n",
      "          [-0.8431, -0.8431, -0.8403,  ..., -0.8353, -0.8353, -0.8353]]]],\n",
      "       dtype=torch.float64), 'targets': tensor([99])}\n"
     ]
    }
   ],
   "source": [
    "for i, data in tqdm(enumerate(test_loader), total=len(test_loader)):         \n",
    "        print(data)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, labels = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-9023643e4b74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'labels'"
     ]
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/raid/data/yanglab/rin3d_downstream/RadImageNet/thyroid-nodule/6451155_20130314091228.104.1454025_right.rst.jp2.png'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['path'][160413]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(619, 855, 3)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image=cv2.imread(df_test['path'][160413])\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[160413]['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "224,224,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "70862b2a35782cf03f229e4805e6826ca21bf4d4920a510b87654639dd617db4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
